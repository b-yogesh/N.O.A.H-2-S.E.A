{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BirdSoundPredictor.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZwZFruIQne3"
      },
      "source": [
        "Inspired from https://github.com/birds-on-mars/birdsonearth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO4ESJLvMVX0"
      },
      "source": [
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from librosa.core import load\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "import resampy\n",
        "from torch.nn.functional import relu, softmax"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ClwWG9cLccS"
      },
      "source": [
        "class Params:\n",
        "    '''\n",
        "    defines all parameters needed in training.py and model.py\n",
        "    TRAINING\n",
        "    n_epochs (int): number of training epochs\n",
        "    batch_size (int):\n",
        "    val_split (float): fraction of data kept for validation\n",
        "    data_root (str): path to data\n",
        "    n_max (int): max number of instances loaded for training\n",
        "    weights (str): path to vggish weights .hdf5 file\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        # Data\n",
        "        #TODO: add options for other formats\n",
        "        self.data_format = 'wav'\n",
        "\n",
        "        # Model\n",
        "        self.n_bins = 64\n",
        "        self.n_frames = 96\n",
        "        self.n_classes = 3\n",
        "\n",
        "        # Training\n",
        "        self.n_epochs = 100\n",
        "        self.batch_size = 512\n",
        "        self.val_split = .2\n",
        "        self.data_root = '../data/full_urbansounds_restructured'\n",
        "        # if mel_spec_root directory exists it is used and preprocessing of data_root is skipped\n",
        "        # otherwise mel specs are computed from data_root\n",
        "        self.mel_spec_root = '../data/full_urbansounds_specs'\n",
        "        self.n_max = None\n",
        "        self.weights = 'models/vggish_audioset_weights_without_fc2.h5'\n",
        "\n",
        "        # model zoo\n",
        "        self.save_model = True\n",
        "        self.model_zoo = 'models'\n",
        "        self.name = 'urban'\n",
        "\n",
        "        # computing device, can be 'cuda:<GPU index>' or 'cpu'\n",
        "        self.device = 'cpu'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DocuPklMJXA"
      },
      "source": [
        "params = Params()\n",
        "\n",
        "params.name = 'BirdsSongs'\n",
        "\n",
        "# training\n",
        "params.data_format = 'wav'\n",
        "params.data_root = '/content/drive/My Drive/ZooHackathon/BirdSong'\n",
        "params.mel_spec_root = '/content/drive/My Drive/ZooHackathon/BirdSong/processed'\n",
        "params.n_epochs = 40\n",
        "params.batch_size = 256\n",
        "params.val_split = .2\n",
        "params.save_model = True\n",
        "params.n_max = 1000"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up13qktwM56Y"
      },
      "source": [
        "class VGGish(nn.Module):\n",
        "\n",
        "    def __init__(self, params):\n",
        "\n",
        "        super(VGGish, self).__init__()\n",
        "\n",
        "        self.n_bins = params.n_bins\n",
        "        self.n_frames = params.n_frames\n",
        "        self.out_dims = int(params.n_bins / 2**4 * params.n_frames / 2**4)\n",
        "        self.n_classes = params.n_classes\n",
        "        self.weights = params.weights\n",
        "        self.model_zoo = params.model_zoo\n",
        "        self.name = params.name\n",
        "\n",
        "        # convolutional bottom part\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=(3, 3), padding=(1, 1))\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "\n",
        "        # fully connected top part\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.out_dims*512, 1028),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1028, 1028),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1028, self.n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        a = self.pool1(relu(self.conv1(X)))\n",
        "        a = self.pool2(relu(self.conv2(a)))\n",
        "        a = relu(self.conv3_1(a))\n",
        "        a = relu(self.conv3_2(a))\n",
        "        a = self.pool3(a)\n",
        "        a = relu(self.conv4_1(a))\n",
        "        a = relu(self.conv4_2(a))\n",
        "        a = self.pool4(a)\n",
        "        a = a.reshape((a.size(0), -1))\n",
        "        a = self.classifier(a)\n",
        "        a = softmax(a)\n",
        "        return a\n",
        "\n",
        "    def init_weights(self, file=None):\n",
        "        '''\n",
        "        laods pretrained weights from an .hdf5 file. File structure must match exactly.\n",
        "        Args:\n",
        "            file (string): path to .hdf5 file containing VGGish weights\n",
        "        '''\n",
        "\n",
        "        if file is not None:\n",
        "            file = file\n",
        "        else:\n",
        "            file = self.weights\n",
        "\n",
        "        # loading weights from file\n",
        "        with h5.File(file, 'r') as f:\n",
        "\n",
        "            conv1 = f['conv1']['conv1']\n",
        "            kernels1 = torch.from_numpy(conv1['kernel:0'][()].transpose(3, 2, 1, 0))\n",
        "            biases1 = torch.from_numpy(conv1['bias:0'][()])\n",
        "            conv2 = f['conv2']['conv2']\n",
        "            kernels2 = torch.from_numpy(conv2['kernel:0'][()].transpose(3, 2, 1, 0))\n",
        "            biases2 = torch.from_numpy(conv2['bias:0'][()])\n",
        "            conv3_1 = f['conv3']['conv3_1']['conv3']['conv3_1']\n",
        "            kernels3_1 = torch.from_numpy(conv3_1['kernel:0'][()].transpose(3, 2, 1, 0))\n",
        "            biases3_1 = torch.from_numpy(conv3_1['bias:0'][()])\n",
        "            conv3_2 = f['conv3']['conv3_2']['conv3']['conv3_2']\n",
        "            kernels3_2 = torch.from_numpy(conv3_2['kernel:0'][()].transpose(3, 2, 1, 0))\n",
        "            biases3_2 = torch.from_numpy(conv3_2['bias:0'][()])\n",
        "            conv4_1 = f['conv4']['conv4_1']['conv4']['conv4_1']\n",
        "            kernels4_1 = torch.from_numpy(conv4_1['kernel:0'][()].transpose(3, 2, 1, 0))\n",
        "            biases4_1 = torch.from_numpy(conv4_1['bias:0'][()])\n",
        "            conv4_2 = f['conv4']['conv4_2']['conv4']['conv4_2']\n",
        "            kernels4_2 = torch.from_numpy(conv4_2['kernel:0'][()].transpose(3, 2, 1, 0))\n",
        "            biases4_2 = torch.from_numpy(conv4_2['bias:0'][()])\n",
        "\n",
        "            # assigning weights to layers\n",
        "            self.conv1.weight.data = kernels1\n",
        "            self.conv1.bias.data = biases1\n",
        "            self.conv2.weight.data = kernels2\n",
        "            self.conv2.bias.data = biases2\n",
        "            self.conv3_1.weight.data = kernels3_1\n",
        "            self.conv3_1.bias.data = biases3_1\n",
        "            self.conv3_2.weight.data = kernels3_2\n",
        "            self.conv3_2.bias.data = biases3_2\n",
        "            self.conv4_1.weight.data = kernels4_1\n",
        "            self.conv4_1.bias.data = biases4_1\n",
        "            self.conv4_2.weight.data = kernels4_2\n",
        "            self.conv4_2.bias.data = biases4_2\n",
        "\n",
        "    def freeze_bottom(self):\n",
        "        '''\n",
        "        freezes the convolutional bottom part of the model.\n",
        "        '''\n",
        "        for layer in self.children():\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                layer.weight.requires_grad = False\n",
        "                layer.bias.requires_grad = False\n",
        "\n",
        "    def save_weights(self):\n",
        "        torch.save(self.state_dict(),\n",
        "                    '/content/drive/My Drive/ZooHackathon/BirdSong/' + self.name+'.pt')\n",
        "        return\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3kewazpNM5v"
      },
      "source": [
        "def preprocess(src, dst):\n",
        "    y, sr = load(src)\n",
        "    y *= 32768\n",
        "    y = y.astype(np.int16)\n",
        "    wavfile.write(dst, rate=22050, data=y)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpU8IaawOQsB"
      },
      "source": [
        "def wavfile_to_examples(wav_file):\n",
        "  \"\"\"Convenience wrapper around waveform_to_examples() for a common WAV format.\n",
        "  Args:\n",
        "    wav_file: String path to a file, or a file-like object. The file\n",
        "    is assumed to contain WAV audio data with signed 16-bit PCM samples.\n",
        "  Returns:\n",
        "    See waveform_to_examples.\n",
        "  \"\"\"\n",
        "  sr, wav_data = wavfile.read(wav_file)\n",
        "  assert wav_data.dtype == np.int16, 'Bad sample type: %r' % wav_data.dtype\n",
        "  samples = wav_data / 32768.0  # Convert to [-1.0, +1.0]\n",
        "  return waveform_to_examples(samples, sr)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqXw7YMqOaRe"
      },
      "source": [
        "def waveform_to_examples(data, sample_rate):\n",
        "  \"\"\"Converts audio waveform into an array of examples for VGGish.\n",
        "  Args:\n",
        "    data: np.array of either one dimension (mono) or two dimensions\n",
        "      (multi-channel, with the outer dimension representing channels).\n",
        "      Each sample is generally expected to lie in the range [-1.0, +1.0],\n",
        "      although this is not required.\n",
        "    sample_rate: Sample rate of data.\n",
        "  Returns:\n",
        "    3-D np.array of shape [num_examples, num_frames, num_bands] which represents\n",
        "    a sequence of examples, each of which contains a patch of log mel\n",
        "    spectrogram, covering num_frames frames of audio and num_bands mel frequency\n",
        "    bands, where the frame length is vggish_params.STFT_HOP_LENGTH_SECONDS.\n",
        "  \"\"\"\n",
        "  # Convert to mono.\n",
        "  if len(data.shape) > 1:\n",
        "    data = np.mean(data, axis=1)\n",
        "  # Resample to the rate assumed by VGGish.\n",
        "  if sample_rate !=  SAMPLE_RATE:\n",
        "    data = resampy.resample(data, sample_rate,  SAMPLE_RATE)\n",
        "\n",
        "  # Compute log mel spectrogram features.\n",
        "  log_mel = log_mel_spectrogram(\n",
        "      data,\n",
        "      audio_sample_rate= SAMPLE_RATE,\n",
        "      log_offset= LOG_OFFSET,\n",
        "      window_length_secs= STFT_WINDOW_LENGTH_SECONDS,\n",
        "      hop_length_secs= STFT_HOP_LENGTH_SECONDS,\n",
        "      num_mel_bins= NUM_MEL_BINS,\n",
        "      lower_edge_hertz= MEL_MIN_HZ,\n",
        "      upper_edge_hertz= MEL_MAX_HZ)\n",
        "\n",
        "  # Frame features into examples.\n",
        "  features_sample_rate = 1.0 /  STFT_HOP_LENGTH_SECONDS\n",
        "  example_window_length = int(round(\n",
        "       EXAMPLE_WINDOW_SECONDS * features_sample_rate))\n",
        "  example_hop_length = int(round(\n",
        "       EXAMPLE_HOP_SECONDS * features_sample_rate))\n",
        "\n",
        "  log_mel_examples = frame(\n",
        "      log_mel,\n",
        "      window_length=example_window_length,\n",
        "      hop_length=example_hop_length)\n",
        "  return log_mel_examples"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWlEA95zOe_g"
      },
      "source": [
        "# Architectural constants.\n",
        "NUM_FRAMES = 96  # Frames in input mel-spectrogram patch.\n",
        "NUM_BANDS = 64  # Frequency bands in input mel-spectrogram patch.\n",
        "EMBEDDING_SIZE = 128  # Size of embedding layer.\n",
        "\n",
        "# Hyperparameters used in feature and example generation.\n",
        "SAMPLE_RATE = 16000\n",
        "STFT_WINDOW_LENGTH_SECONDS = 0.025\n",
        "STFT_HOP_LENGTH_SECONDS = 0.010\n",
        "NUM_MEL_BINS = NUM_BANDS\n",
        "MEL_MIN_HZ = 125\n",
        "MEL_MAX_HZ = 7500\n",
        "LOG_OFFSET = 0.01  # Offset used for stabilized log of input mel-spectrogram.\n",
        "EXAMPLE_WINDOW_SECONDS = 0.96  # Each example contains 96 10ms frames\n",
        "EXAMPLE_HOP_SECONDS = 0.96     # with zero overlap.\n",
        "\n",
        "# Parameters used for embedding postprocessing.\n",
        "PCA_EIGEN_VECTORS_NAME = 'pca_eigen_vectors'\n",
        "PCA_MEANS_NAME = 'pca_means'\n",
        "QUANTIZE_MIN_VAL = -2.0\n",
        "QUANTIZE_MAX_VAL = +2.0\n",
        "\n",
        "# Hyperparameters used in training.\n",
        "INIT_STDDEV = 0.01  # Standard deviation used to initialize weights.\n",
        "LEARNING_RATE = 1e-4  # Learning rate for the Adam optimizer.\n",
        "ADAM_EPSILON = 1e-8  # Epsilon for the Adam optimizer.\n",
        "\n",
        "# Names of ops, tensors, and features.\n",
        "INPUT_OP_NAME = 'vggish/input_features'\n",
        "INPUT_TENSOR_NAME = INPUT_OP_NAME + ':0'\n",
        "OUTPUT_OP_NAME = 'vggish/embedding'\n",
        "OUTPUT_TENSOR_NAME = OUTPUT_OP_NAME + ':0'\n",
        "AUDIO_EMBEDDING_FEATURE_NAME = 'audio_embedding'\n",
        "\n",
        "_MEL_BREAK_FREQUENCY_HERTZ = 700.0\n",
        "_MEL_HIGH_FREQUENCY_Q = 1127.0\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7woILqJVOstP"
      },
      "source": [
        "def log_mel_spectrogram(data,\n",
        "                        audio_sample_rate=8000,\n",
        "                        log_offset=0.0,\n",
        "                        window_length_secs=0.025,\n",
        "                        hop_length_secs=0.010,\n",
        "                        **kwargs):\n",
        "  \"\"\"Convert waveform to a log magnitude mel-frequency spectrogram.\n",
        "  Args:\n",
        "    data: 1D np.array of waveform data.\n",
        "    audio_sample_rate: The sampling rate of data.\n",
        "    log_offset: Add this to values when taking log to avoid -Infs.\n",
        "    window_length_secs: Duration of each window to analyze.\n",
        "    hop_length_secs: Advance between successive analysis windows.\n",
        "    **kwargs: Additional arguments to pass to spectrogram_to_mel_matrix.\n",
        "  Returns:\n",
        "    2D np.array of (num_frames, num_mel_bins) consisting of log mel filterbank\n",
        "    magnitudes for successive frames.\n",
        "  \"\"\"\n",
        "  window_length_samples = int(round(audio_sample_rate * window_length_secs))\n",
        "  hop_length_samples = int(round(audio_sample_rate * hop_length_secs))\n",
        "  fft_length = 2 ** int(np.ceil(np.log(window_length_samples) / np.log(2.0)))\n",
        "  spectrogram = stft_magnitude(\n",
        "      data,\n",
        "      fft_length=fft_length,\n",
        "      hop_length=hop_length_samples,\n",
        "      window_length=window_length_samples)\n",
        "  mel_spectrogram = np.dot(spectrogram, spectrogram_to_mel_matrix(\n",
        "      num_spectrogram_bins=spectrogram.shape[1],\n",
        "      audio_sample_rate=audio_sample_rate, **kwargs))\n",
        "  return np.log(mel_spectrogram + log_offset)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPoXNAP2PNlH"
      },
      "source": [
        "def frame(data, window_length, hop_length):\n",
        "  \"\"\"Convert array into a sequence of successive possibly overlapping frames.\n",
        "  An n-dimensional array of shape (num_samples, ...) is converted into an\n",
        "  (n+1)-D array of shape (num_frames, window_length, ...), where each frame\n",
        "  starts hop_length points after the preceding one.\n",
        "  This is accomplished using stride_tricks, so the original data is not\n",
        "  copied.  However, there is no zero-padding, so any incomplete frames at the\n",
        "  end are not included.\n",
        "  Args:\n",
        "    data: np.array of dimension N >= 1.\n",
        "    window_length: Number of samples in each frame.\n",
        "    hop_length: Advance (in samples) between each window.\n",
        "  Returns:\n",
        "    (N+1)-D np.array with as many rows as there are complete frames that can be\n",
        "    extracted.\n",
        "  \"\"\"\n",
        "  num_samples = data.shape[0]\n",
        "  num_frames = 1 + int(np.floor((num_samples - window_length) / hop_length))\n",
        "  shape = (num_frames, window_length) + data.shape[1:]\n",
        "  strides = (data.strides[0] * hop_length,) + data.strides\n",
        "  return np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8voUKSj1PGX-"
      },
      "source": [
        "def stft_magnitude(signal, fft_length,\n",
        "                   hop_length=None,\n",
        "                   window_length=None):\n",
        "  \"\"\"Calculate the short-time Fourier transform magnitude.\n",
        "  Args:\n",
        "    signal: 1D np.array of the input time-domain signal.\n",
        "    fft_length: Size of the FFT to apply.\n",
        "    hop_length: Advance (in samples) between each frame passed to FFT.\n",
        "    window_length: Length of each block of samples to pass to FFT.\n",
        "  Returns:\n",
        "    2D np.array where each row contains the magnitudes of the fft_length/2+1\n",
        "    unique values of the FFT for the corresponding frame of input samples.\n",
        "  \"\"\"\n",
        "  frames = frame(signal, window_length, hop_length)\n",
        "  # Apply frame window to each frame. We use a periodic Hann (cosine of period\n",
        "  # window_length) instead of the symmetric Hann of np.hanning (period\n",
        "  # window_length-1).\n",
        "  window = periodic_hann(window_length)\n",
        "  windowed_frames = frames * window\n",
        "  return np.abs(np.fft.rfft(windowed_frames, int(fft_length)))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmEe4c9ePRIm"
      },
      "source": [
        "def periodic_hann(window_length):\n",
        "  \"\"\"Calculate a \"periodic\" Hann window.\n",
        "  The classic Hann window is defined as a raised cosine that starts and\n",
        "  ends on zero, and where every value appears twice, except the middle\n",
        "  point for an odd-length window.  Matlab calls this a \"symmetric\" window\n",
        "  and np.hanning() returns it.  However, for Fourier analysis, this\n",
        "  actually represents just over one cycle of a period N-1 cosine, and\n",
        "  thus is not compactly expressed on a length-N Fourier basis.  Instead,\n",
        "  it's better to use a raised cosine that ends just before the final\n",
        "  zero value - i.e. a complete cycle of a period-N cosine.  Matlab\n",
        "  calls this a \"periodic\" window. This routine calculates it.\n",
        "  Args:\n",
        "    window_length: The number of points in the returned window.\n",
        "  Returns:\n",
        "    A 1D np.array containing the periodic hann window.\n",
        "  \"\"\"\n",
        "  return 0.5 - (0.5 * np.cos(2 * np.pi / window_length *\n",
        "                             np.arange(window_length)))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj7r0l9pPWt-"
      },
      "source": [
        "def spectrogram_to_mel_matrix(num_mel_bins=20,\n",
        "                              num_spectrogram_bins=129,\n",
        "                              audio_sample_rate=8000,\n",
        "                              lower_edge_hertz=125.0,\n",
        "                              upper_edge_hertz=3800.0):\n",
        "  \"\"\"Return a matrix that can post-multiply spectrogram rows to make mel.\n",
        "  Returns a np.array matrix A that can be used to post-multiply a matrix S of\n",
        "  spectrogram values (STFT magnitudes) arranged as frames x bins to generate a\n",
        "  \"mel spectrogram\" M of frames x num_mel_bins.  M = S A.\n",
        "  The classic HTK algorithm exploits the complementarity of adjacent mel bands\n",
        "  to multiply each FFT bin by only one mel weight, then add it, with positive\n",
        "  and negative signs, to the two adjacent mel bands to which that bin\n",
        "  contributes.  Here, by expressing this operation as a matrix multiply, we go\n",
        "  from num_fft multiplies per frame (plus around 2*num_fft adds) to around\n",
        "  num_fft^2 multiplies and adds.  However, because these are all presumably\n",
        "  accomplished in a single call to np.dot(), it's not clear which approach is\n",
        "  faster in Python.  The matrix multiplication has the attraction of being more\n",
        "  general and flexible, and much easier to read.\n",
        "  Args:\n",
        "    num_mel_bins: How many bands in the resulting mel spectrum.  This is\n",
        "      the number of columns in the output matrix.\n",
        "    num_spectrogram_bins: How many bins there are in the source spectrogram\n",
        "      data, which is understood to be fft_size/2 + 1, i.e. the spectrogram\n",
        "      only contains the nonredundant FFT bins.\n",
        "    audio_sample_rate: Samples per second of the audio at the input to the\n",
        "      spectrogram. We need this to figure out the actual frequencies for\n",
        "      each spectrogram bin, which dictates how they are mapped into mel.\n",
        "    lower_edge_hertz: Lower bound on the frequencies to be included in the mel\n",
        "      spectrum.  This corresponds to the lower edge of the lowest triangular\n",
        "      band.\n",
        "    upper_edge_hertz: The desired top edge of the highest frequency band.\n",
        "  Returns:\n",
        "    An np.array with shape (num_spectrogram_bins, num_mel_bins).\n",
        "  Raises:\n",
        "    ValueError: if frequency edges are incorrectly ordered or out of range.\n",
        "  \"\"\"\n",
        "  nyquist_hertz = audio_sample_rate / 2.\n",
        "  if lower_edge_hertz < 0.0:\n",
        "    raise ValueError(\"lower_edge_hertz %.1f must be >= 0\" % lower_edge_hertz)\n",
        "  if lower_edge_hertz >= upper_edge_hertz:\n",
        "    raise ValueError(\"lower_edge_hertz %.1f >= upper_edge_hertz %.1f\" %\n",
        "                     (lower_edge_hertz, upper_edge_hertz))\n",
        "  if upper_edge_hertz > nyquist_hertz:\n",
        "    raise ValueError(\"upper_edge_hertz %.1f is greater than Nyquist %.1f\" %\n",
        "                     (upper_edge_hertz, nyquist_hertz))\n",
        "  spectrogram_bins_hertz = np.linspace(0.0, nyquist_hertz, num_spectrogram_bins)\n",
        "  spectrogram_bins_mel = hertz_to_mel(spectrogram_bins_hertz)\n",
        "  # The i'th mel band (starting from i=1) has center frequency\n",
        "  # band_edges_mel[i], lower edge band_edges_mel[i-1], and higher edge\n",
        "  # band_edges_mel[i+1].  Thus, we need num_mel_bins + 2 values in\n",
        "  # the band_edges_mel arrays.\n",
        "  band_edges_mel = np.linspace(hertz_to_mel(lower_edge_hertz),\n",
        "                               hertz_to_mel(upper_edge_hertz), num_mel_bins + 2)\n",
        "  # Matrix to post-multiply feature arrays whose rows are num_spectrogram_bins\n",
        "  # of spectrogram values.\n",
        "  mel_weights_matrix = np.empty((num_spectrogram_bins, num_mel_bins))\n",
        "  for i in range(num_mel_bins):\n",
        "    lower_edge_mel, center_mel, upper_edge_mel = band_edges_mel[i:i + 3]\n",
        "    # Calculate lower and upper slopes for every spectrogram bin.\n",
        "    # Line segments are linear in the *mel* domain, not hertz.\n",
        "    lower_slope = ((spectrogram_bins_mel - lower_edge_mel) /\n",
        "                   (center_mel - lower_edge_mel))\n",
        "    upper_slope = ((upper_edge_mel - spectrogram_bins_mel) /\n",
        "                   (upper_edge_mel - center_mel))\n",
        "    # .. then intersect them with each other and zero.\n",
        "    mel_weights_matrix[:, i] = np.maximum(0.0, np.minimum(lower_slope,\n",
        "                                                          upper_slope))\n",
        "  # HTK excludes the spectrogram DC bin; make sure it always gets a zero\n",
        "  # coefficient.\n",
        "  mel_weights_matrix[0, :] = 0.0\n",
        "  return mel_weights_matrix"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYqEUP8WPb2T"
      },
      "source": [
        "def hertz_to_mel(frequencies_hertz):\n",
        "  \"\"\"Convert frequencies to mel scale using HTK formula.\n",
        "  Args:\n",
        "    frequencies_hertz: Scalar or np.array of frequencies in hertz.\n",
        "  Returns:\n",
        "    Object of same size as frequencies_hertz containing corresponding values\n",
        "    on the mel scale.\n",
        "  \"\"\"\n",
        "  return _MEL_HIGH_FREQUENCY_Q * np.log(\n",
        "      1.0 + (frequencies_hertz / _MEL_BREAK_FREQUENCY_HERTZ))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G66AX7qwMLEr"
      },
      "source": [
        "def load_model_with(params, path, ptPath):\n",
        "    print('loading model')\n",
        "    # load class labels\n",
        "    with open(path, 'rb') as f:\n",
        "        labels = pickle.load(f)\n",
        "    # init network and load weights\n",
        "    params.n_classes = len(labels)\n",
        "    device = torch.device(params.device)\n",
        "    net = VGGish(params)\n",
        "    new_top = torch.nn.Linear(net.out_dims*512, net.n_classes)\n",
        "    net.classifier = new_top\n",
        "    net.load_state_dict(torch.load(ptPath,\n",
        "                        map_location=device))\n",
        "    net.to(device)\n",
        "    net.eval()\n",
        "    print('model for labels {} is ready'.format(labels))\n",
        "    return net, labels\n",
        "\n",
        "\n",
        "def predict(net, labels, files, params):\n",
        "    print('starting inference')\n",
        "    device = torch.device(params.device)\n",
        "    predictions = []\n",
        "    probs = []\n",
        "    for i, file in enumerate(files):\n",
        "        filename = os.path.splitext(os.path.basename(file))[0]\n",
        "        processed = filename + '_proc.wav'\n",
        "        preprocess(file, processed)\n",
        "        data = wavfile_to_examples(processed)\n",
        "        data = torch.from_numpy(data).unsqueeze(1).float()\n",
        "        data = data.to(device)\n",
        "        net.to(device)\n",
        "        out = net(data)\n",
        "        # # for each spectrogram/row index of max probability\n",
        "        # pred = np.argmax(out.detach().cpu().numpy(), axis=1)\n",
        "        # # find most frequent index over all spectrograms\n",
        "        # consensus = np.bincount(pred).argmax()\n",
        "        # print('file {} sounds like a {} to me'.format(i, labels[consensus]))\n",
        "        # mean probabilities for each col/class over all spectrograms\n",
        "        mean_probs = np.mean(out.detach().cpu().numpy(), axis=0)\n",
        "        # find index of max mean_probs\n",
        "        idx = np.argmax(mean_probs)\n",
        "        print('file {} sounds like a {} to me'.format(i, labels[idx]))\n",
        "        print('my guesses are: ')\n",
        "        for j, label in enumerate(labels):\n",
        "            print('{0}: {1:.04f}'.format(label, mean_probs[j]))\n",
        "        # predictions.append(labels[consensus])\n",
        "        predictions.append(labels[idx])\n",
        "        probs.append(mean_probs)\n",
        "        os.remove(processed)\n",
        "    return predictions, probs\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmNDK0GqLjN4"
      },
      "source": [
        "def run_prediction(input):\n",
        "    net, labels = load_model_with(params, '/content/drive/My Drive/ZooHackathon/BirdSong/BirdSongClassifier.pkl', '/content/drive/My Drive/ZooHackathon/BirdSong/BirdsSongs.pt')\n",
        "    predictions, probs = predict(net, labels, input, params)\n",
        "    return predictions, probs"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afxw_NObLqbD"
      },
      "source": [
        "input = ['/content/drive/My Drive/ZooHackathon/BirdSong/Rusty_Blackbird/544790.wav']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgadzREsMTSt",
        "outputId": "14f0dada-b711-4bb0-9ee8-c23bde828321",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predictions, probs = run_prediction(input)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading model\n",
            "model for labels ['.ipynb_checkpoints', 'Crested_Auklet', 'Least_Auklet', 'Parakeet_Auklet', 'Rusty_Blackbird'] is ready\n",
            "starting inference\n",
            "file 0 sounds like a Rusty_Blackbird to me\n",
            "my guesses are: \n",
            ".ipynb_checkpoints: 0.0003\n",
            "Crested_Auklet: 0.0017\n",
            "Least_Auklet: 0.0146\n",
            "Parakeet_Auklet: 0.0003\n",
            "Rusty_Blackbird: 0.9831\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhUXiKOkMqo5",
        "outputId": "19691636-8ebc-464d-992e-51c00f6fdab5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predictions, probs"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Rusty_Blackbird'],\n",
              " [array([2.7955859e-04, 1.6797802e-03, 1.4578681e-02, 3.2238371e-04,\n",
              "         9.8313934e-01], dtype=float32)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAfSh9IMP68l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}